---
title: "LR Theory"
author: "Tim Radtke"
header-includes:
   #- \documentclass[a4paper,twoside,english]{article}
   - \usepackage[retainorgcmds]{IEEEtrantools}
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{bbm}
   - \newtheorem{theorem}{Theorem}
   - \newtheorem{lemma}{Lemma}
   - \newcommand{\KL}{\,\text{KL}}
   - \newcommand{\der}{\,\text{d}}
   #- \usepackage[top=1.5cm, bottom=1.5cm, outer=5cm, inner=2cm, heightrounded, marginparwidth=2.5cm, marginparsep=2cm]{geometry}
   #- \usepackage{fontspec}
   #-   \setmainfont{TimesNewRoman}
keep_tex: TRUE
output:
  pdf_document:
    number_sections: true
    keep_tex: true
    #toc: true
    #toc_depth: 3
    #latex_engine: xelatex
fontsize: 12pt
urlcolor: blue
---

# Kullback-Leibler Divergence Strategy

We start with a definition of the Likelihood Ratio. See for example Example 6 in [http://math.arizona.edu/~jwatkins/s-lrt.pdf](http://math.arizona.edu/~jwatkins/s-lrt.pdf).

In our strategy, we will have at time $t$ a sample of observations for arm $i$ of size $T_i(t)$: $\mathbb{x} = x_1, ..., x_{T_i(t)}$. Since we are only considering Bernoulli distributions at this point, the Likelihood for a sample from distribution $Ber(\mu)$ is given by

$$ 
L(\mu|\mathbb{x}) = (1-\mu)^{T_i(t) - \sum_{s = 1}^{T_i(t)} x_s} \mu^{\sum_{s = 1}^{T_i(t)} x_s}
$$

We want to test whether the sample $\mathbb{x}$ with maximum likelihood estimate ($H_1: \mu = \hat{\mu}$) is actually different from the standard hypothesis that it is equal to the threshold $\tau$ ($H_0: \mu = \tau$). To test this, we can use the likelihood ratio:

$$
\Lambda = \frac{L(\tau|\mathbb{x})}{L(\hat{\mu}|\mathbb{x})} = \frac{\tau^{\hat{\mu}T_i(t)}(1-\tau)^{(1-\hat{\mu})T_i(t)} }{\hat{\mu}^{\hat{\mu}T_i(t)}(1-\hat{\mu})^{(1-\hat{\mu})T_i(t)}}
$$
If the likelihood ratio is large, then it's quite likely that the sample was generated by a Bernoulli distribution with parameter $\tau$. If the ratio is small, then the maximum likelihood estimate is very different from the threshold. The statistic should be smaller than 1 given that we compare to the maximum likelihood estimate.

In an adaptive sampling strategy that is based on this ratio, it is natural to pull the arm that currently maximizes the ratio (an arm with distribution close to $\tau$). Even if the true parameter $\mu$ is close to $\tau$, the ratio will increase over time as $T_i(t)$ increases. Thus given two distributions with the same maximum likelihood estimate, the strategy will pull the arm that has a smaller $T_i(t)$. 

Thus our strategy could be to pull arm $i^*$ at time $t+1$ that maximizes $\tilde{B}_i(t) = \Lambda_i$. Of course, $\arg \max_i \Lambda_i(t) = \arg \min_i -\log(\Lambda_i(t))$. And so an equivalent strategy is to pull $i^*(t) = \arg \min_i -\log(\Lambda_i(t))$. We now show that this is equivalent to $i^*(t) = \arg \min_i T_i(t)\hat{\KL}(\hat{\mu}||\tau)$.

We have

\begin{align*}
-\log(\Lambda_i(t)) & = \hat{\mu}T_i(t) \log(\frac{\hat{\mu}_i(t)}{\tau}) + (1-\hat{\mu})T_i(t) \log(\frac{1-\hat{\mu}_i(t)}{1-\tau}) \\
& = T_i(t) [\hat{\mu}_i(t) \log(\frac{\hat{\mu}_i(t)}{\tau}) + (1-\hat{\mu}_i(t)) \log(\frac{1-\hat{\mu}_i(t)}{1-\tau})] \\
& = T_i(t) \der(\hat{\mu}_i(t), \tau) \\
& = B_i(t)
\end{align*}

This notation of the index will prove useful in theoretical derivations, while the likelihood ratio is advantageous in the implementation of the algorithm, as it can be computed even when $\hat{\mu}=0$ or $\hat{\mu}=1$, which will always occur in the beginning of the sampling.

## Comparison with Other Indices

If we consider $B_i^{APT}(t) = T_i(t) \der(\hat{\mu}_i(t), \tau)$, we see a similarity in the index that first showed up in Locatelli et al. with the APT strategy where $B^{APT}_i(t) = \sqrt{T_i(t)} \Delta_i(t) = \sqrt{T_i(t)} |\mu_i - \tau|$ (if we consider the case where $\epsilon = 0$). This format for the strategy's index was inherited by Zhong et al. (2017) for their EVT algorithm which adapts both the term estimating the difference between $\hat{\mu}$ and $\tau$, which I will now refer to as $D_k(t)$, and the confidence term of the current estimate. In Zhong et al., the index is given by $B_i^{EVT}(t) = D_i^{EVT}(t)S^{EVT}_i(t)$, where the difference term $D_i^{EVT}(t) = D_i^{APT}(t) = |\mu_i(t) - \tau|$, but where the confidence term $S_i^{EVT}(t)$ is no longer equal to $S_i^{APT}(t)=\sqrt{T_i(t)}$ but $S_i^{EVT}(t) = \Big(\frac{a}{T_i(t)} + \sqrt{\frac{a}{T_i(t)}}\hat{\sigma}_i(t)\Big)$, and thus depends on the current estimate of the variance, $\hat{\sigma}_i(t)$. Continuing this format of indices, the strategy proposed above has an index that can be written as

$$
B_i(t) = S^{KL}_i(t) D_i^{KL}(t) = T_i(t) \der(\hat{\mu}_i(t), \tau)
$$

If we write the Kullback-Leibler divergence as approximated by a Normal distribution, $\der(\hat{\mu}, \tau) \approx \frac{(\hat{\mu}-\tau)^2}{\tau(1-\tau)}$ or using Pinsker's inequality to bound it as $\der(\hat{\mu}, \tau) \geq 2|\hat{\mu}-\tau|^2$, we see that these approximations lead to the index that is used by the APT strategy. Thus, the motivation behind the likelihood ratio based strategy is to improve upon APT for cases where the approximation fails. We see that in the case of Bernoulli distributions, the two strategies are equivalent for $\tau = 0.5$.

## Upper Bound for KL Strategy

Given the general schema for the index in both the APT and the EVT strategy, the proofs for their respective upper bound on the expected loss share share certain steps. Both proofs build on favorable events which hold with large probability for certain levels of $T$, $T_i$, $H$. In both proofs, the authors bound the number of pulls that a helpful arm receives; that is, an arm that received at least a share of the budget $T$ that is proportional to his contribution to the overall complexity $H$, and which is played in the final round $T$. Both strategies need to show that on the favorable event, after $T$ rounds, every arm has been pulled a suffient amount of times for $\hat{\mu}_i(T)$ to be seperated sufficiently from $\tau$ to make a correct classification of arm $i$. In the design of the favorable event, both strategies can use the fact that the $|\hat{\mu} - \tau|$ term in the index get the following upper bound $|\hat{\Delta}_i(t) - \Delta_i| \leq |\hat{\mu}_i(t)-\mu_i|$ on the $D_i^{APT}(t) = D_i^{EVT}(t) = \hat{\Delta}_i(t)$ part of the index. Given this bound, both proofs use a favorable event on which $\hat{\mu}$ is very close to $\mu$ with large probability.

Given that the Kullback-Leibler divergence is not a metric, and more specifically, the triangle inequality does not hold, the favorable event from the other proofs will not suffice to bound $D_i^{KL}(t)$. 

Given the lower bound on $T_k(t)$ for the helpful arm $k$, and the lower and upper bound on $\hat{\Delta}_k(t)$ and $\hat{\Delta}_i(t)$ through the favorable event (compare equation (10) in A.2 of Locatelli et al.), all that remains to show the sufficiently seperated $\hat{\mu}$ from $\tau$ is to lower bound the number of times the not helpful arms are pulled, $T_i(t)$. Locatelli et al. can derive this easily from the other bounds by comparing the two indices $B_k(t)$ and $B_i(t)$. Since at time $t$ the arm $k$ has been pulled, the APT strategy has $B_k^{APT}(t) \leq B_i^{APT}(t)$, since the algorithm pulls the $\arg \min$. Similar holds for the EVT algorithm. And using the Kullback-Leibler distance instead of the Likelihood-ratio, the KL based strategy also minimizes $B_i(t)$ (instead of maximizing $\tilde{B}_i(t)$.

Thus, for the KL-based strategy, consider again for the helpful arm $k$ pulled at time $t$, and some other arm $i$:

$$
B_k(t) \leq B_i(t) \Leftrightarrow T_k(t) \der(\hat{\mu}_k(t), \tau) \leq T_i(t) \der(\hat{\mu}_i(t), \tau)
$$

Starting from here, our goal will be to show that some kind of seperation of the arms holds on the favorable event, where the favorable event is set up such that the above quantities are bounded and the seperation condition can be checked. On the way, the favorable event will have an impact on how the complexity $H$ will be defined for this problem. In the end, we want to derive an upper bound of rate $\exp(-\frac{T}{H})$ on the error probability.