---
title: "LR Theory"
author: "Tim Radtke"
header-includes:
   #- \documentclass[a4paper,twoside,english]{article}
   - \usepackage[retainorgcmds]{IEEEtrantools}
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{bbm}
   - \newtheorem{theorem}{Theorem}
   - \newtheorem{lemma}{Lemma}
   - \newcommand{\KL}{\,\text{KL}}
   - \newcommand{\der}{\,\text{d}}
   #- \usepackage[top=1.5cm, bottom=1.5cm, outer=5cm, inner=2cm, heightrounded, marginparwidth=2.5cm, marginparsep=2cm]{geometry}
   #- \usepackage{fontspec}
   #-   \setmainfont{TimesNewRoman}
keep_tex: TRUE
output:
  pdf_document:
    number_sections: true
    keep_tex: true
    #toc: true
    #toc_depth: 3
    #latex_engine: xelatex
fontsize: 12pt
urlcolor: blue
---

# Kullback-Leibler Divergence Strategy

We start with a definition of the Likelihood Ratio. See for example Example 6 in [http://math.arizona.edu/~jwatkins/s-lrt.pdf](http://math.arizona.edu/~jwatkins/s-lrt.pdf).

In our strategy, we will have at time $t$ a sample of observations for arm $i$ of size $T_i(t)$: $X = x_1, ..., x_{T_i(t)}$. Since we are only considering Bernoulli distributions at this point, the Likelihood for a sample from distribution $Ber(\mu)$ is given by

$$ 
L(\mu|X) = (1-\mu)^{T_i(t) - \sum_{s = 1}^{T_i(t)} x_s} \mu^{\sum_{s = 1}^{T_i(t)} x_s}
$$

We want to test whether the sample $X$ with maximum likelihood estimate ($H_1: \mu = \hat{\mu}$) is actually different from the standard hypothesis that it is equal to the threshold $\tau$ ($H_0: \mu = \tau$). To test this, we can use the likelihood ratio:

$$
\Lambda = \frac{L(\tau|X)}{L(\hat{\mu}|X)} = \frac{\tau^{\hat{\mu}T_i(t)}(1-\tau)^{(1-\hat{\mu})T_i(t)} }{\hat{\mu}^{\hat{\mu}T_i(t)}(1-\hat{\mu})^{(1-\hat{\mu})T_i(t)}}
$$

If the likelihood ratio is large, then it's quite likely that the sample was generated by a Bernoulli distribution with parameter $\tau$. If the ratio is small, then the maximum likelihood estimate is very different from the threshold. The statistic should be smaller than 1 given that we compare to the maximum likelihood estimate.

In an adaptive sampling strategy that is based on this ratio, it is natural to pull the arm that currently maximizes the ratio (an arm with distribution close to $\tau$). Even if the true parameter $\mu$ is close to $\tau$, the ratio will increase over time as $T_i(t)$ increases. Thus given two distributions with the same maximum likelihood estimate, the strategy will pull the arm that has a smaller $T_i(t)$. 

Thus our strategy could be to pull arm $i^*$ at time $t+1$ that maximizes $\tilde{B}_i(t) = \Lambda_i$. Of course, $\arg \max_i \Lambda_i(t) = \arg \min_i -\log(\Lambda_i(t))$. And so an equivalent strategy is to pull $i^*(t) = \arg \min_i -\log(\Lambda_i(t))$. We now show that this is equivalent to $i^*(t) = \arg \min_i T_i(t)\hat{\KL}(\hat{\mu}||\tau)$.

We have

\begin{align*}
-\log(\Lambda_i(t)) & = \hat{\mu}T_i(t) \log(\frac{\hat{\mu}_i(t)}{\tau}) + (1-\hat{\mu})T_i(t) \log(\frac{1-\hat{\mu}_i(t)}{1-\tau}) \\
& = T_i(t) [\hat{\mu}_i(t) \log(\frac{\hat{\mu}_i(t)}{\tau}) + (1-\hat{\mu}_i(t)) \log(\frac{1-\hat{\mu}_i(t)}{1-\tau})] \\
& = T_i(t) \der(\hat{\mu}_i(t), \tau) \\
& = B_i(t)
\end{align*}

This notation of the index will prove useful in theoretical derivations, while the likelihood ratio is advantageous in the implementation of the algorithm, as it can be computed even when $\hat{\mu}=0$ or $\hat{\mu}=1$, which will always occur in the beginning of the sampling.

## Comparison with Other Indices

If we consider $B_i^{KL}(t) = T_i(t) \der(\hat{\mu}_i(t), \tau)$, we see a similarity to the index that first showed up in Locatelli et al. (2016) with the APT strategy where $B^{APT}_i(t) = \sqrt{T_i(t)} \Delta_i(t) = \sqrt{T_i(t)} |\mu_i(t) - \tau|$ (if we consider the case where $\epsilon = 0$). This format for the strategy's index was inherited by [Zhong et al. (2017)](https://arxiv.org/pdf/1704.04567.pdf) for their EVT algorithm which adapts both the term estimating the difference between $\hat{\mu}$ and $\tau$, which I will now refer to as $D_k(t)$, and the confidence term of the current estimate, which they call $S_i^{EVT}(t)$. In Zhong et al., the index is given by $B_i^{EVT}(t) = D_i^{EVT}(t)S^{EVT}_i(t)$, where the difference term $D_i^{EVT}(t) = D_i^{APT}(t) = |\mu_i(t) - \tau|$, but where the confidence term $S_i^{EVT}(t)$ is no longer equal to $S_i^{APT}(t)=\sqrt{T_i(t)}$ but $S_i^{EVT}(t) = \Big(\frac{a}{T_i(t)} + \sqrt{\frac{a}{T_i(t)}}\hat{\sigma}_i(t)\Big)$, and thus depends on the current estimate of the variance, $\hat{\sigma}_i(t)$. Continuing this format of indices, the strategy proposed above has an index that can be written as

$$
B_i(t) = S^{KL}_i(t) D_i^{KL}(t) = T_i(t) \der(\hat{\mu}_i(t), \tau)
$$

If we write the Kullback-Leibler divergence as approximated by a Normal distribution, $\der(\hat{\mu}, \tau) \approx \frac{(\hat{\mu}-\tau)^2}{\tau(1-\tau)}$ or using Pinsker's inequality to bound it as $\der(\hat{\mu}, \tau) \geq 2|\hat{\mu}-\tau|^2$, we see that these approximations lead to the index that is used by the APT strategy. Thus, the motivation behind the likelihood ratio based strategy is to improve upon APT for cases where the approximation fails. We see that in the case of Bernoulli distributions, the two strategies are equivalent for $\tau = 0.5$.

## Upper Bound for KL Strategy

Given the general schema for the index in both the APT and the EVT strategy, the proofs for their respective upper bound on the expected loss share certain steps. Both proofs build on favorable events which hold with large probability for certain levels of $T$, $T_i$, $H$. In both proofs, the authors bound the number of pulls that a helpful arm receives; that is, an arm that received at least a share of the budget $T$ that is proportional to his contribution to the overall complexity $H$, and which is played in the final round $T$. Both strategies need to show that on the favorable event, after $T$ rounds, every arm has been pulled a suffient amount of times for $\hat{\mu}_i(T)$ to be seperated sufficiently from $\tau$ to make a correct classification of arm $i$. In the design of the favorable event, both strategies can use the fact that the $|\hat{\mu} - \tau|$ term in the index get the following upper bound $|\hat{\Delta}_i(t) - \Delta_i| \leq |\hat{\mu}_i(t)-\mu_i|$ on the $D_i^{APT}(t) = D_i^{EVT}(t) = \hat{\Delta}_i(t)$ part of the index. Given this bound, both proofs use a favorable event on which $\hat{\mu}$ is very close to $\mu$ with large probability.

Given that the Kullback-Leibler divergence is not a metric, and more specifically, the triangle inequality does not hold, the favorable event from the other proofs will not suffice to bound $D_i^{KL}(t)$. 

Given the lower bound on $T_k(t)$ for the helpful arm $k$, and the lower and upper bound on $\hat{\Delta}_k(t)$ and $\hat{\Delta}_i(t)$ through the favorable event (compare equation (10) in A.2 of Locatelli et al., 2016), all that remains to show the sufficiently seperated $\hat{\mu}$ from $\tau$ is to lower bound the number of times the not helpful arms are pulled, $T_i(t)$. Locatelli et al. can derive this easily from the other bounds by comparing the two indices $B_k(t)$ and $B_i(t)$. Since at time $t$ the arm $k$ has been pulled, the APT strategy has $B_k^{APT}(t) \leq B_i^{APT}(t)$, since the algorithm pulls the $\arg \min$. Similar holds for the EVT algorithm. And using the Kullback-Leibler distance instead of the Likelihood-ratio, the KL based strategy also minimizes $B_i(t)$ (instead of maximizing $\tilde{B}_i(t)$.

Thus, for the KL-based strategy, consider again for the helpful arm $k$ pulled at time $t$, and some other arm $i$:

$$
B_k(t) \leq B_i(t) \Leftrightarrow T_k(t) \der(\hat{\mu}_k(t), \tau) \leq T_i(t) \der(\hat{\mu}_i(t), \tau)
$$

Starting from here, our goal will be to show that some kind of seperation of the arms holds on the favorable event, where the favorable event is set up such that the above quantities are bounded and the seperation condition can be checked. On the way, the favorable event will have an impact on how the complexity $H$ will be defined for this problem. In the end, we want to derive an upper bound of rate $\exp(-\frac{T}{H})$ on the error probability.

To achieve correct seperation at the end of the algorithm, [Zhong et al. (2017)](https://arxiv.org/pdf/1704.04567.pdf) require that after the last round $T$, the sample means $\hat{\mu}$ of the arms are closer to the true mean $\mu$ of the distribution than to the threshold $\tau$:

$$
|\hat{\mu}_i(T_i(T)) - \mu| \leq \frac{\Delta_i}{2}
$$

More explicitly, for an arm below the threshold, $\mu_i < \tau$, they demand $\hat{\mu}_i(T_i(T)) \geq \mu_i - \frac{\mu_i-\tau}{2}$. On the other hand, for an arm above the threshold, $\mu_i \geq \tau$, they demand $\hat{\mu}_i(T_i(T)) \leq \mu_i - \frac{\mu_i-\tau}{2}$.

Based on a similar idea, the fixed-confidence KL-LUCB algorithm introduced in [Kaufmann and Kalyanakrishnan](http://proceedings.mlr.press/v30/Kaufmann13.pdf) (2013) for the Top-$m$ problem employs a stopping rule that checks whether the smallest lower bound among the currently best $m$ algorithms is larger than the largest upper bound of the other arms that currently are not among the $m$ best. If the lower bound is larger than the upper bound, the two sets of arms are considered to be sufficiently seperated; it is unlikely, that there are two arms that are classified into the wrong set (compare with Proposition 1, [Kaufmann and Kalyanakrishnan](http://proceedings.mlr.press/v30/Kaufmann13.pdf), 2013).

Thus a simple demand that we can pose for our strategy is the one from Zhong et al. (2017), which directly translates into 

$$
\der(\hat{\mu}_i(T_i(T)), \tau) \geq \der(\mu_i +\frac{\tau-\mu_i}{2}, \tau)
$$

for arms with mean below the threshold, $\mu_i < \tau$. On the other hand, for arms with mean above the threshold, $\mu_i > \tau$, we demand

$$
\der(\hat{\mu}_i(T_i(T)), \tau) \geq \der(\mu_i - \frac{\mu_i - \tau}{2}, \tau).
$$

These two conditions are equivalent to the conditions given in [Zhong et al. (2017)](https://arxiv.org/pdf/1704.04567.pdf) due to the monotonicity of the KL divergence. If we could check whether these conditions hold on whatever favorable event we use, that would be a very convenient check.

UPDATE: This translation into the Kullback-Leibler distance does not work as easily, because the above conditions might hold even if $\mu < \tau < \hat{\mu}$, for example.

### The Favorable Event

Intuitively, it should be an event of large probability that $|\der(\mu,\tau) - \der(\hat{\mu},\tau)|<\epsilon$, given that $|\mu - \hat{\mu}|<\delta$ is likely. Also since we know that $\der(\hat{\mu}(t), \mu) \leq \epsilon$ is likely for large $t$ following for example Proposition 1 in [Cappé et al. (2013)](https://projecteuclid.org/download/pdfview_1/euclid.aos/1375362558). Additionally we know that $\der(\hat{\mu}, \tau) = \der(\mu, \tau)$ for $\hat{mu} = \mu$, however otherwise the inequality  $\der(\hat{\mu}, \tau) \geq \der(\hat{\mu}, \mu) + \der(\mu, \tau)$ only holds for $\mu \in [\hat{\mu}, \tau]$ (or $\mu \in [\tau, \hat{\mu}]$).

Instead, we will try to base our favorable event on a variant of Lemma 1 in [Kaufmann and Kalyanakrishnan](http://proceedings.mlr.press/v30/Kaufmann13.pdf) (2013), which states the following.

Let $T \geq 1$ be an integer. Let $\delta > 0$, $\gamma > 0$, and $c \in (0,1)$ be such that $mu_a \neq \tau$.

$$
\sum_{t=1}^T \mathbb{P} \Big(a = u_t \lor a = l_t, T_a(t) > \left \lceil{\frac{\gamma}{d^*(\mu_a,\tau)}}\right \rceil, T_a(t) d(\hat{\mu}_a, \tau) \leq \gamma \Big) \leq \frac{exp(-\gamma)}{d^*(\mu_a, \tau)}
$$

Given Lemma 2 in the same paper, as well as the proof of Lemma 1 in Appendix C of the paper (last inequality), the same bound should hold if we write the event without the lower and upper bounds $u_t$ and $l_t$ of arm $a$:

$$
\sum_{t=1}^T \mathbb{P} \Big(T_a(t) > \left \lceil{\frac{\gamma}{d^*(\mu_a,\tau)}}\right \rceil, T_a(t) d(\hat{\mu}_a, \tau) \leq \gamma \Big) \leq \frac{exp(-\gamma)}{d^*(\mu_a, \tau)}
$$

Thus on the favorable event for a single arm $a$, we have that

$$
T_a(t) -1 \geq \frac{\gamma}{d^*(\mu_a, \tau)} \qquad \text{and} \qquad d(\hat{\mu}_a, \tau) > \frac{\gamma}{T_a(t)}
$$

### Characterization of some helpful arm

Locatelli et al. (2016) show for arm $k$ that $T_k(t) \geq T_k(T) - 1 \geq \frac{T}{2H_{APT}\Delta^2_{APT,k}}$. We expect (CHECK THIS) that a similar condition holds for our algorithm, however with a different $H_{KL}$ and $h_{KL,k}$, $H_{KL} = \sum_{i=1}^K h_{KL,i}$, $h_{APT,k} = 1/(|\hat{\mu}-\tau|+\epsilon)^2$; but where the helpful arm $k$ still has at least as many pulls as would be its share of the budget given its contribution to the overall complexity.

Now, we can go ahead and plug the favorable event directly into $B_k(t)$ (on the favorable event, it is likely that $\gamma < T_a(t) d(\hat{\mu}_a, \tau)$):

$$
B_k(t) \leq B_i(t) \Leftrightarrow \gamma < T_k(t) \der(\hat{\mu}_k(t), \tau) \leq T_i(t) \der(\hat{\mu}_i(t), \tau) 
$$

To check whether this makes sense, we need to choose a value for $\gamma$, such that the second part of the favorable event is fulfilled. Given the lower bound for the helpful arm, we can define the complexity in terms of the Chernoff information used in [Kaufmann and Kalyanakrishnan](http://proceedings.mlr.press/v30/Kaufmann13.pdf) (2013). We write

$$
H_{KL} = \sum_{i=1}^K \frac{1}{d^*(\mu_i, \tau)}
$$

For two Bernoullis, the Chernoff information is defined as $d^*(\mu_i, \tau) = d(z^*,\mu_i) = d(z^*, \tau)$ (compare [Kaufmann and Kalyanakrishnan](http://proceedings.mlr.press/v30/Kaufmann13.pdf), 2013). For the helpful arm we thus get

$$
T_k(t) \geq T_k(T) -1 \geq \frac{T}{2 H_{KL} d^*(\mu_i, \tau)}
$$
such that by choosing $\gamma = \frac{T}{2H_{KL}}$ the favorable event $T_a(t) -1 > \frac{\gamma}{d^*(\mu_a, \tau)}$ works for the helpful arm:

$$
\gamma = \frac{T}{2H_{KL}} \leq T_k(t) \der(\hat{\mu}_k(t), \tau) \leq T_i(t) \der(\hat{\mu}_i(t), \tau) 
$$

Given that we can conclude the proof successfully, this choice of the exploration exponent $\gamma$ already implies that the budget should be rather large if $H_{KL}$ is large. But it also implies that the favorable event occurs at the rate that we demanded upfront, since then $exp(-\gamma) = exp(-T/(2H))$.

### Problem

At this point, however, I am stuck since the favorable event does not give an upper bound on the RHS of the equation. Thus I don't have a way of bounding $T_i(t)$. I tried to approximate the KL-divergence $d(\hat{\mu}_k(t), \tau) \leq \frac{(\hat{\mu}-\tau)^2}{\tau(1-\tau)}$ to get a bound similarly as in Locatelli et al. (2016) using the favorable event mentioned there; this gave me indeed a lower bound on $T_i(t)$ similar to the one in that paper. However, given that the favorable event bounds the KL divergence from below as $\frac{\gamma}{T_a(t)} < d(\hat{\mu}_a, \tau)$, the lower bound for $T_i(t)$ did not prove useful. Instead, it might have showed me another problem. At first I believed it would suffice to show that $d(\hat{\mu}_a, \tau)$ is larger than e.g. $d^*(\mu_a, \tau)$ on the favorable event. But in theory, this could also hold if $\mu_a < \tau$ and $\tau < \hat{\mu}_a$. So a check against that probably needs to be included in the favorable event as well. This might be possible with some proposition from the [KL-UCB paper by Cappé et al. (2013)](https://projecteuclid.org/download/pdfview_1/euclid.aos/1375362558), or a self-normalized inequality by [Garivier and Cappé (2011, Theorem 10)](https://arxiv.org/pdf/1102.2490.pdf). 

In general, I'm simply wondering which inequality might suffice to bound $B_k(t) < B_i(t)$. I believed that the one from [Kaufmann and Kalyanakrishnan](http://proceedings.mlr.press/v30/Kaufmann13.pdf) sufficed, but in their paper it is combined with a nice separation condition stemming from the lower and upper bounds, which is integrated into the stopping rule of the fixed-*confidence* algorithm (where the separation thus has to hold).

## Experiments

First, we again show the result of experiment 3. This time around, however, we set $\epsilon = 0$ as recommended by Andrea. Also, we move the two arms that previously have been inside the $\epsilon$-interval a little away from the threshold $\tau = 0.5$ to remove the need for a very large sample size. The likelihood-ratio based algorithm is now able to show it's advantage.

The setup is as follows:

```{r, message = FALSE, echo = FALSE}
get_APT_complexity <- function(means, tau, epsilon) {
  res <- vector(length = length(means))
  for(i in 1:length(means)) {
    res[i] <- (abs(means[i] - tau) + epsilon)^2
  }
  sum(1/res)
}

mean_loc6nt <- c(0.001, 0.005, 0.01, 0.015,
                 0.04, 0.06,
                 0.085, 0.09, 0.095, 0.099)
tau_loc6nt <- 0.05
epsilon_loc6nt <- 0
H_loc6nt <- get_APT_complexity(mean_loc6nt, tau_loc6nt, epsilon_loc6nt)

print(mean_loc6nt)

plot(mean_loc6nt, rep(1,10), main = paste0("APT Complexity of ", round(H_loc6nt,2)))
abline(v=tau_loc6nt)
```

```{r, echo = FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
current_path <- "/Users/timradtke/Dropbox/1Master/Master Thesis/threshold-thesis/synthetic_experiments/"

load(paste0(current_path, "/loc6nt_comp_BUCB_horizon_7000.Rda"))
load(paste0(current_path, "/loc6nt_comp_UNIFORM_7000.Rda"))
load(paste0(current_path, "/loc6nt_comp_APT_7000.Rda"))
load(paste0(current_path, "/loc6nt_comp_LR_7000.Rda"))

alg_names <- c(rep(c("Uniform", "APT", "B-UCB", "LR"), each = 6991))
index <- c(rep(10:7000, times = 4))
alg_res <- log(c(loc6nt_comp_UNIFORM_7000, loc6nt_comp_APT_7000, loc6nt_comp_BUCB_horizon_7000, loc6nt_comp_LR_7000))
loc3_comp_res <- data.frame(index = index, names = alg_names, 
                            results = alg_res)
ggplot(loc3_comp_res, aes(x = index, y = alg_res)) + geom_line(aes(group = alg_names, color = alg_names)) + geom_hline(aes(yintercept = log(0.1)), linetype = 2) + geom_hline(aes(yintercept = log(0.01)), linetype = 2) +
  labs(title = "Experiment 3 Results", subtitle = paste0("APT Complexity of ", round(H_loc6nt),  ". Horizontal lines at 0.1 and 0.01."), y = "log(Error)")
```

### Experiment 2

This is a new setup in which the observations are set in more or less the same distance from each other on a log10 scale. Again, $\epsilon = 0$. We choose a very small threshold with $\tau = 0.01$. Also note that we have increased the budget to $T=10000$, while the previous experiment had $T=7000$.

```{r, message = FALSE, echo = FALSE}
get_APT_complexity <- function(means, tau, epsilon) {
  res <- vector(length = length(means))
  for(i in 1:length(means)) {
    res[i] <- (abs(means[i] - tau) + epsilon)^2
  }
  sum(1/res)
}

mean_loc7nt <- c(10^-4, 10^-3.5, 10^-3.25, 10^-3, 
                 10^-2.75, 10^-2.5, 10^-2.25, 10^-1.85, 10^-1.75,
                 10^-1.5, 10^-1)
tau_loc7nt <- 10^-2
epsilon_loc7nt <- 0
H_loc7nt <- get_APT_complexity(mean_loc7nt, tau_loc7nt, epsilon_loc7nt)

print(round(mean_loc7nt,5))

plot(log10(mean_loc7nt), rep(1,11), main = paste0("Complexity of ", round(H_loc7nt,2)), ylab = "", xlab = "log10(mean)")
abline(v=log10(tau_loc7nt))
```

```{r, echo = FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
current_path <- "/Users/timradtke/Dropbox/1Master/Master Thesis/threshold-thesis/synthetic_experiments/"

load(paste0(current_path, "/loc7nt_comp_BUCB_horizon_10000.Rda"))
load(paste0(current_path, "/loc7nt_comp_UNIFORM_10000.Rda"))
load(paste0(current_path, "/loc7nt_comp_APT_10000.Rda"))
load(paste0(current_path, "/loc7nt_comp_LR_10000.Rda"))
load(paste0(current_path, "/loc7nt_comp_AugUCB_10000.Rda"))

alg_names <- c(rep(c("Uniform", "APT", "B-UCB", "LR", "AugUCB"), each = 9990))
index <- c(rep(11:10000, times = 5))
alg_res <- log(c(loc7nt_comp_UNIFORM_10000, loc7nt_comp_APT_10000, loc7nt_comp_BUCB_horizon_10000, loc7nt_comp_LR_10000, loc7nt_comp_AugUCB_10000))
loc3_comp_res <- data.frame(index = index, names = alg_names, 
                            results = alg_res)
ggplot(loc3_comp_res, aes(x = index, y = alg_res)) + geom_line(aes(group = alg_names, color = alg_names)) + geom_hline(aes(yintercept = log(0.1)), linetype = 2) + geom_hline(aes(yintercept = log(0.01)), linetype = 2) +
  labs(title = "Experiment Results", subtitle = paste0("APT Complexity of ", round(H_loc7nt),  ". Horizontal lines at 0.1 and 0.01."), y = "log(Error)")
```